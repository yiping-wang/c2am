{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import voc12.dataloader\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from misc import pyutils, torchutils\n",
    "from net.resnet50_cam import CAM, Net, ReCAM    \n",
    "from misc import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list = '/Users/Andy/Projects/c2am/voc12/train_aug.txt'\n",
    "voc12_root = '/Users/Andy/Projects/dataset/VOCdevkit/VOC2012'\n",
    "baseline_cam_weight_path = '/Users/Andy/Projects/resnet50_baseline_512.pth'\n",
    "\n",
    "cls_dataset = voc12.dataloader.VOC12ClassificationDataset(train_list, voc12_root=voc12_root,\n",
    "                                                            resize_long=(320, 640), hor_flip=True,\n",
    "                                                            crop_size=512, crop_method=\"random\")\n",
    "cls_data_loader = DataLoader(cls_dataset,\n",
    "                               batch_size=1,\n",
    "                               shuffle=False,\n",
    "                               num_workers=0)\n",
    "\n",
    "\n",
    "cam_dataset = voc12.dataloader.VOC12ClassificationDatasetMSF(train_list,\n",
    "                                                          voc12_root=voc12_root,\n",
    "                                                          scales=(1.0, 0.5, 1.5, 2.0))\n",
    "cam_data_loader = DataLoader(cam_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "cls_model = Net()\n",
    "cam_model = CAM()\n",
    "cls_model.eval()\n",
    "cam_model.eval()\n",
    "cls_model.load_state_dict(torch.load(baseline_cam_weight_path, map_location='cpu'))\n",
    "cam_model.load_state_dict(torch.load(baseline_cam_weight_path, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = {}\n",
    "labels = np.zeros((1, 20))\n",
    "for pack in cls_data_loader:\n",
    "    label = pack['label'].numpy()\n",
    "    labels += label\n",
    "#     if label not in label_set:\n",
    "#         label_set[label] = 0\n",
    "#     label_set[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pack in tqdm(cls_data_loader):\n",
    "#     names = pack['name']\n",
    "#     imgs = pack['img']\n",
    "#     labels = pack['label']\n",
    "#     if labels[0][14] and labels[0][12] == 0:\n",
    "#         logits, _ = cls_model(imgs)\n",
    "#         probs = F.softmax(logits, dim=1)\n",
    "#         if probs[0][12] > 0.2:\n",
    "#             print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in label_set.items():\n",
    "    if k[5] or k[14]:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_wrong = []\n",
    "\n",
    "for pack in data_loader:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 5, 14]),)\n",
      "person\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "CAT_LIST = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "            'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "            'cow', 'diningtable', 'dog', 'horse',\n",
    "            'motorbike', 'person', 'pottedplant',\n",
    "            'sheep', 'sofa', 'train',\n",
    "            'tvmonitor']\n",
    "idx = np.asarray((0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0))\n",
    "print(np.nonzero(idx))\n",
    "print(CAT_LIST[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andy/Projects/front_door_cam/.env/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "for pack in data_loader:\n",
    "    label = pack['label'].numpy().tolist()[0]\n",
    "    size = pack['size']\n",
    "    if label[5] or label[14]:\n",
    "        with torch.no_grad():\n",
    "            strided_up_size = imutils.get_strided_up_size(size, 16)\n",
    "            baseline_outputs = [model_baseline(img[0]) for img in pack['img']]\n",
    "            baseline_highres_cam = [F.interpolate(torch.unsqueeze(\n",
    "                        o, 1), strided_up_size, mode='bilinear', align_corners=False) for o in baseline_outputs]\n",
    "            baseline_highres_cam = torch.sum(torch.stack(baseline_highres_cam, 0), 0)[\n",
    "                        :, 0, :size[0], :size[1]]\n",
    "            valid_cat = torch.nonzero(pack['label'][0])[:, 0]\n",
    "            baseline_highres_cam /= F.adaptive_max_pool2d(baseline_highres_cam, (1, 1)) + 1e-5\n",
    "\n",
    "        w = size[0].item()\n",
    "        h = size[1].item()\n",
    "        fig = plt.figure(figsize=(192.0,108.0))\n",
    "        columns = 20 + 1\n",
    "        rows = 1\n",
    "        n = 20\n",
    "        for i in range(1, n +1):\n",
    "            img = baseline_highres_cam[i-1]\n",
    "            img = img.unsqueeze(0).unsqueeze(0)\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(img.numpy().squeeze())\n",
    "            plt.title(CAT_LIST[i-1])\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(Image.open(os.path.join(voc12_root, 'JPEGImages') + '/' + pack['name'][0] + '.jpg'))\n",
    "        plt.title(pack['name'][0])\n",
    "        fig.savefig('../cam_vis/{}.png'.format(pack['name'][0]), dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
